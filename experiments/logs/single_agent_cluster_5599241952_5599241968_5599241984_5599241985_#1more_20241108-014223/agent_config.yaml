activation: relu
batch_size: 192
double_dqn: true
dropout_rate: 0.12
dueling_dqn: true
epsilon_decay: 0.93
epsilon_min: 0.03
epsilon_start: 1.0
gamma: 0.96
gradient_clip: 0.8
hidden_size: 384
learning_rate: 0.0004
learning_rate_decay: 0.998
memory_size: 150000
min_learning_rate: 0.0001
num_hidden_layers: 3
prioritized_replay: true
priority_alpha: 0.65
priority_beta: 0.45
target_update_frequency: 75
