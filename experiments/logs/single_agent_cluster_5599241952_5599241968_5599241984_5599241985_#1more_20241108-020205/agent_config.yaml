activation: relu
batch_size: 64
double_dqn: true
dropout_rate: 0.1
dueling_dqn: true
epsilon_decay: 0.95
epsilon_min: 0.05
epsilon_start: 1.0
gamma: 0.95
gradient_clip: 0.5
hidden_size: 256
learning_rate: 0.0001
learning_rate_decay: 0.999
memory_size: 100000
min_learning_rate: 1.0e-05
num_hidden_layers: 2
prioritized_replay: true
priority_alpha: 0.6
priority_beta: 0.4
target_update_frequency: 50
