# config/state_sharing_config.yaml

# Environment Configuration
env:
  net_file: "baseline/osm.net.xml.gz"
  route_file: "baseline/osm.passenger.trips.xml"
  use_gui: false
  episode_length: 3600  # 1 hour in seconds, matching Baseline
  delta_time: 1  # Match Baseline's step length
  min_green_time: 10
  max_green_time: 60
  yellow_time: 2

# Agent Configuration
agent:
  # DQN parameters
  gamma: 0.99  # Slightly increased discount factor
  learning_rate: 0.003  # Increased learning rate for faster adaptation
  epsilon_start: 1.0
  epsilon_min: 0.05  # Reduced minimum exploration
  epsilon_decay: 0.985  # Slower decay to maintain exploration
  batch_size: 128  # Increased batch size
  memory_size: 500000  # Larger replay memory
  hidden_sizes: [256, 128]  # Deeper network
  target_update_frequency: 50  # More frequent target network updates

  # Advanced features
  double_dqn: true
  dueling_dqn: true
  prioritized_replay: true

# Training Configuration
training:
  num_episodes: 20
  max_steps: 600
  save_frequency: 10
  
  # Early stopping
  early_stopping: true
  patience: 10
  min_improvement: 0.05  # More stringent improvement criteria

  # Experience sharing
  enable_experience_sharing: false
  share_batch_size: 32
  sharing_frequency: 1

# Hierarchical Structure
hierarchical:
  enable_regions: false  # Simplified
  region_update_frequency: 1
  regional_reward_weight: 0.0
  neighbor_reward_weight: 0.0
  local_reward_weight: 1.0

# Reward weights
rewards:
  waiting_time: -2.0  # Increased penalty
  queue_length: -1.5  # Stronger queue penalty
  speed: 0.5  # Moderate speed reward
  emergency: 0.0
  throughput: 1.0  # Increased throughput reward

# Logging Configuration
logging:
  log_dir: "results/state_sharing"
  model_dir: "models/state_sharing"
  log_frequency: 100

  # Simplified metrics
  metrics:
    - "mean_waiting_time"
    - "throughput"
    - "average_speed"
    - "completed_trips"

# Visualization
visualization:
  enable_tensorboard: false
  plot_frequency: 0
  save_plots: false
  plot_types: []